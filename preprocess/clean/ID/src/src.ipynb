{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dropping status date because I don't know what they mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tbl():\n",
    "    df = pd.read_csv(\"../data/input/EmployeePublicRecordRequestReport.csv\")\n",
    "    return df \n",
    "\n",
    "df = read_tbl()\n",
    "\n",
    "\n",
    "### proceed with cleaning\n",
    "\n",
    "def clean_agency(df):\n",
    "    df.loc[:, \"agency_name\"] = (df\n",
    "                           .agency_name\n",
    "                           .str.lower()\n",
    "                           .str.strip()\n",
    "                           .str.replace(r\"dept\\.\", \"department\", regex=True)\n",
    "                           .str.replace(r\"&\", \"and\", regex=False)\n",
    "                           .str.replace(r\"\\bpd\\b\", \"police department\", regex=True)\n",
    "                           .str.replace(r\"(\\w+) \\s+(\\w+)\", r\"\\1 \\2\", regex=True)\n",
    "    )\n",
    "    df = df[~((df.agency_name.str.contains(\"fire\")))]\n",
    "    return df\n",
    "\n",
    "def rename_cols(df):\n",
    "    df = df.rename(columns={\"First Name\": \"first_name\", \"Last Name\": \"last_name\", \n",
    "                            \"Start Date\": \"start_date\", \"End Date\": \"end_date\",\n",
    "                            \"End Action\": \"separation_reason\", \"Agency\": \"agency_name\", \n",
    "                            \"Certification\": \"certification_type\", \"Status\": \"employment_status\",\n",
    "                            \"POST ID\": \"person_nbr\"})\n",
    "    return df\n",
    "\n",
    "def drop_duplicate_rows(df):\n",
    "    df = df.drop_duplicates(subset=[\"person_nbr\", \"start_date\", \"end_date\"])\n",
    "    return df \n",
    "\n",
    "\n",
    "def clean_date(date_str):\n",
    "    try:\n",
    "        year = int(date_str[:4])\n",
    "        if year < 1800 or year > 2100:  # Adjust the range as needed\n",
    "            return None\n",
    "        return date_str\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def collapse_contiguous_stints(\n",
    "    df: pd.DataFrame, bycols=[\"person_nbr\", \"agency_name\"]\n",
    ") -> pd.DataFrame:\n",
    "    # Convert dates to datetime format first\n",
    "    df.loc[:, \"start_date\"] = pd.to_datetime(df.start_date)\n",
    "    df.loc[:, \"end_date\"] = pd.to_datetime(df.end_date)\n",
    "    \n",
    "    # assume missing end dates are current employment, and use today's date for sorting purposes\n",
    "    import datetime  # Make sure to import datetime module\n",
    "    \n",
    "    one_day = pd.to_timedelta(1, \"days\")\n",
    "    today = pd.to_datetime(datetime.datetime.today(), utc=False)  # Fixed: use datetime.datetime.today()\n",
    "    ancient = pd.to_datetime(\"1800-01-01\", utc=False)\n",
    "    working = df.sort_values([\"person_nbr\", \"agency_name\", \"start_date\"], inplace=False)\n",
    "    \n",
    "    # No need to apply clean_date and convert again since we already have datetime objects\n",
    "    # working[\"start_date\"] = working[\"start_date\"].apply(clean_date)\n",
    "    # working[\"end_date\"] = working[\"end_date\"].apply(clean_date)\n",
    "    # working[\"start_date\"] = pd.to_datetime(working.start_date, utc=False).fillna(ancient)\n",
    "    # working[\"end_date\"] = pd.to_datetime(working.end_date, utc=False).fillna(today)\n",
    "    \n",
    "    # Just fill missing values\n",
    "    working[\"start_date\"] = working[\"start_date\"].fillna(ancient)\n",
    "    working[\"end_date\"] = working[\"end_date\"].fillna(today)\n",
    "    \n",
    "    working.loc[working.start_date < ancient, \"start_date\"] = ancient\n",
    "    working.loc[working.end_date > today, \"end_date\"] = today\n",
    "    grouped = working.groupby(bycols)\n",
    "    working[\"prv_end\"] = grouped[\"end_date\"].shift(1, fill_value=today)\n",
    "    working[\"new_stint\"] = (working.start_date - working.prv_end) > one_day\n",
    "    working[\"stint_id\"] = grouped[\"new_stint\"].cumsum()\n",
    "    collapsible = working.groupby(bycols + [\"stint_id\"])\n",
    "    summaries = {k: lambda x: x.tail(1) for k in df.columns if k not in bycols}\n",
    "    summaries[\"start_date\"] = \"min\"\n",
    "    summaries[\"end_date\"] = \"max\"\n",
    "    out = collapsible.aggregate(summaries).reset_index()\n",
    "    out[\"start_date\"] = out[\"start_date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "    out[\"end_date\"] = out[\"end_date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "    out.loc[out.end_date == today.strftime(\"%Y-%m-%d\"), \"end_date\"] = None\n",
    "    out.loc[out.start_date == ancient.strftime(\"%Y-%m-%d\"), \"start_date\"] = None\n",
    "    return out.drop([\"stint_id\"], axis=1, inplace=False)\n",
    "\n",
    "\n",
    "\n",
    "df = df.pipe(rename_cols).pipe(drop_duplicate_rows)\n",
    "\n",
    "\n",
    "df.loc[:, \"full_name\"] = df.first_name + \" \" + df.last_name\n",
    "\n",
    "df = df.pipe(collapse_contiguous_stints)\n",
    "\n",
    "df.to_csv(\"../data/output/id-index-2025.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"../data/output/idaho_index_new.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
